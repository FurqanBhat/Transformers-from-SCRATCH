{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FurqanBhat/Transformers-from-SCRATCH/blob/main/transformers_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854b1c26",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-10T00:21:41.601447Z",
          "iopub.status.busy": "2025-06-10T00:21:41.601237Z",
          "iopub.status.idle": "2025-06-10T00:23:06.733320Z",
          "shell.execute_reply": "2025-06-10T00:23:06.732554Z"
        },
        "papermill": {
          "duration": 85.136556,
          "end_time": "2025-06-10T00:23:06.734796",
          "exception": false,
          "start_time": "2025-06-10T00:21:41.598240",
          "status": "completed"
        },
        "tags": [],
        "id": "854b1c26",
        "outputId": "1f0ad9e6-dde0-4e09-eeef-7d11ab4472cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\r\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\r\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\r\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\r\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\r\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\r\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\r\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\r\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\r\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\r\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\r\n",
            "Collecting fsspec (from torch)\r\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\r\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.3.8)\r\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.2.4)\r\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (0.1.1)\r\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2025.1.0)\r\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2022.1.0)\r\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2.4.1)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\r\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2022.1.0)\r\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics) (1.3.0)\r\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
            "  Attempting uninstall: nvidia-curand-cu12\r\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
            "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
            "  Attempting uninstall: nvidia-cufft-cu12\r\n",
            "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
            "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
            "  Attempting uninstall: nvidia-cublas-cu12\r\n",
            "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
            "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
            "  Attempting uninstall: fsspec\r\n",
            "    Found existing installation: fsspec 2025.3.2\r\n",
            "    Uninstalling fsspec-2025.3.2:\r\n",
            "      Successfully uninstalled fsspec-2025.3.2\r\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
            "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchmetrics transformers datasets tokenizers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928447ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-10T00:23:06.787594Z",
          "iopub.status.busy": "2025-06-10T00:23:06.787286Z",
          "iopub.status.idle": "2025-06-10T00:23:38.486070Z",
          "shell.execute_reply": "2025-06-10T00:23:38.485436Z"
        },
        "papermill": {
          "duration": 31.726639,
          "end_time": "2025-06-10T00:23:38.487518",
          "exception": false,
          "start_time": "2025-06-10T00:23:06.760879",
          "status": "completed"
        },
        "tags": [],
        "id": "928447ea",
        "outputId": "e918174d-30a6-4a2e-8cf2-1d1bff550d47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-10 00:23:12.728296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749514992.972094      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749514993.041212      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# All Imports\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchmetrics\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "# ==============================================================================\n",
        "# config.py\n",
        "# ==============================================================================\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 20,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 350,\n",
        "        \"d_model\": 512,\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config[\"model_folder\"]\n",
        "    model_basename = config[\"model_basename\"]\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# ==============================================================================\n",
        "# Causal Mask (used by both dataset.py and train.py)\n",
        "# ==============================================================================\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "# ==============================================================================\n",
        "# model.py\n",
        "# ==============================================================================\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps: float = 10**-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return transformer\n",
        "\n",
        "# ==============================================================================\n",
        "# dataset.py\n",
        "# ==============================================================================\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Corrected a bug from your original file: used tokenizer_tgt for target text\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        encoder_input = torch.cat([\n",
        "            self.sos_token,\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
        "        ], dim=0)\n",
        "\n",
        "        decoder_input = torch.cat([\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "        ], dim=0)\n",
        "\n",
        "        # ----------------- THIS IS THE FIX -----------------\n",
        "        # The number of padding tokens for the label must ensure its total length is seq_len.\n",
        "        label = torch.cat([\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "        ], dim=0)\n",
        "        # ---------------------------------------------------\n",
        "\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,\n",
        "            \"decoder_input\": decoder_input,\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "            \"label\": label,\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# train.py\n",
        "# ==============================================================================\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "    console_width = 80\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "            print_msg('-' * console_width)\n",
        "            print_msg(f\"SOURCE: {source_text}\")\n",
        "            print_msg(f\"TARGET: {target_text}\")\n",
        "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
        "            if count == num_examples:\n",
        "                break\n",
        "    if writer:\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=1)\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            label = batch['label'].to(device)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adff541",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-10T00:23:38.541683Z",
          "iopub.status.busy": "2025-06-10T00:23:38.541174Z",
          "iopub.status.idle": "2025-06-10T10:12:29.307903Z",
          "shell.execute_reply": "2025-06-10T10:12:29.307098Z"
        },
        "papermill": {
          "duration": 35330.795592,
          "end_time": "2025-06-10T10:12:29.309630",
          "exception": false,
          "start_time": "2025-06-10T00:23:38.514038",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "83b0f254321c482db34a9c1dcf546053",
            "59e4a4edbe1140b389233eaf581fad66",
            "5ac3cac3d0414e5597727cba6bd98d42"
          ]
        },
        "id": "9adff541",
        "outputId": "1ea03b79-1078-4600-c237-eb02c81d9bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b0f254321c482db34a9c1dcf546053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59e4a4edbe1140b389233eaf581fad66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ac3cac3d0414e5597727cba6bd98d42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 00:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 00: 100%|██████████| 3638/3638 [29:11<00:00,  2.08it/s, loss=3.921]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Perhaps.\n",
            "TARGET: — Sì, può darsi....\n",
            "PREDICTED: — E il signor Rochester .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"The child ought to have change of air and scene,\" he added, speaking to himself; \"nerves not in a good state.\"\n",
            "TARGET: — Per questa bimba ci vorrebbe un cambiamento d'aria e di luogo — aggiunse come se parlasse a sè stesso. I suoi nervi non sono in buono stato.\n",
            "PREDICTED: — , e la mia voce , — disse , — disse la mia vita , — non è più .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 01:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 01: 100%|██████████| 3638/3638 [29:26<00:00,  2.06it/s, loss=5.319]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The large, beautiful, perfectly regular shape of the horse, with his wonderful hindquarters and his exceptionally short pasterns just above his hoofs, involuntarily arrested Vronsky's attention.\n",
            "TARGET: Le forme grandi, stupende, del tutto regolari dello stallone dal dorso magnifico e le giunture straordinariamente corte proprio al di sopra degli zoccoli, fermarono involontariamente l’attenzione di Vronskij.\n",
            "PREDICTED: Il fiume , , che il suo viso , , e , , , a Levin , che aveva fatto il suo marito .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: He wished to glance round but dared not do so, and he tried to keep calm and not to urge his mare, but to let her retain a reserve of strength such as he felt that Gladiator still had.\n",
            "TARGET: Voleva voltarsi indietro a guardare, ma non osava, e cercava di calmarsi e di non lanciare la cavalla per non sciupare in essa una riserva di forze eguale a quella che sentiva in Gladiator.\n",
            "PREDICTED: Egli si sentiva che il suo marito , ma non aveva mai potuto , ma non aveva mai potuto , ma non aveva mai potuto .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 02:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 02: 100%|██████████| 3638/3638 [29:21<00:00,  2.07it/s, loss=3.605]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: As a child that has been hurt skips about, making its muscles move in order to dull its pain, so Karenin needed mental activity to smother those thoughts about his wife which in her presence and in the presence of Vronsky, and amid the continual mention of his name, forced themselves upon him.\n",
            "TARGET: Come un bambino che, dopo aver urtato in qualche cosa, mette in moto, saltando, i propri muscoli per soffocare il dolore, così Aleksej Aleksandrovic aveva bisogno di un moto intellettuale per soffocare quei suoi pensieri sulla moglie che ora, alla presenza di lei e alla presenza di Vronskij, e alla continua ripetizione del nome di lui, urgevano perché si prestasse loro attenzione.\n",
            "PREDICTED: E , come è un uomo che si , come un uomo che , come si , come se la sua vita , Aleksej Aleksandrovic , che la sua vita , la sua vita , la sua vita , e la sua vita , che la sua vita , la sua vita , la sua vita , la sua vita , che la sua vita , la sua vita , la sua , la sua vita , la sua vita , la sua , la sua vita , la sua vita , la sua vita , la sua , la sua vita , la sua vita , la sua vita , la sua , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua , la sua , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , che la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , che la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , che la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , la sua vita , che la sua vita , la sua vita , che la sua vita , che la sua vita , la sua vita , la sua vita , la\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: But this hope of mine was soon taken away; for when he went to sea, he left me on shore to look after his little garden, and do the common drudgery of slaves about his house; and when he came home again from his cruise, he ordered me to lie in the cabin to look after the ship.\n",
            "TARGET: Ma questa mia speranza dovè cessare bentosto, perchè quand’egli si rimise in mare, mi lasciò su la spiaggia per custodirgli il suo piccolo giardino, e dedicarmi alle solite fatiche di schiavo nella sua casa; quando tornò dal suo corseggiare, mi pose nella camera del suo legno corsaro per farvi la guardia.\n",
            "PREDICTED: Ma che la mia abitazione era stata , che il mio moschetto , quando il mio moschetto , quando il mio moschetto , e la mia casa , che , che , quando il mio moschetto , che , la mia abitazione , e la mia abitazione , che la mia abitazione .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 03:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 03: 100%|██████████| 3638/3638 [29:20<00:00,  2.07it/s, loss=4.892]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'No, not one, I swear.\n",
            "TARGET: — Eh, via, neppure una.\n",
            "PREDICTED: — No , non ho bisogno di Dio !\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: This place I was obliged to leave four days before I came here.\n",
            "TARGET: Fui obbligata di lasciare quel posto quattro giorni prima di venire qui.\n",
            "PREDICTED: Questo giorno mi fu molto difficile che io avessi potuto .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 04:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 04: 100%|██████████| 3638/3638 [29:21<00:00,  2.06it/s, loss=4.965]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"Well, but what?\"\n",
            "TARGET: — Altro che?\n",
            "PREDICTED: — Ebbene , ma che cosa ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: “No shoot,” says Friday, “no yet; me shoot now, me no kill; me stay, give you one more laugh:” and, indeed, so he did; for when the bear saw his enemy gone, he came back from the bough, where he stood, but did it very cautiously, looking behind him every step, and coming backward till he got into the body of the tree, then, with the same hinder end foremost, he came down the tree, grasping it with his claws, and moving one foot at a time, very leisurely.\n",
            "TARGET: Venerdì saltò tanto, e i corrispondenti atti dell’orso furono tanto grotteschi, che avemmo campo a ridere per un bel pezzo Dopo di che andato all’estrema punta del ramo, laddove poteva farlo piegare col proprio peso vi si attaccò, e lasciandosi bellamente calar giù finchè fosse vicino a terra abbastanza per ispiccare un salto, eccolo su due piedi e presso al suo moschetto di cui si munì, ma lasciandolo tuttavia ozioso. — «Orsù dunque, Venerdì, gli diss’io, che cosa state a fare adesso? Perchè non gli tirate? — Non ancora; adesso allora, ripetè; me non ammazzare lui adesso, me fermarmi qui; me darvi sempre più bel ridere».\n",
            "PREDICTED: « Oh , Venerdì , che non mi , nè mi , e mi più di nuovo , e che a me , quando gli occhi di nuovo , quando , quando , quando , quando , quando , quando , quando , quando , , , si , e , , e , , , e , , , e , , , , , e , , , , e , , , e , e , , e , e , , , , , , e , e , , , , , , , , , e , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 05:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 05: 100%|██████████| 3638/3638 [29:23<00:00,  2.06it/s, loss=4.972]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Another thing was that, having read a great many books he became convinced that those who shared his outlook understood only what he had understood, explaining nothing and merely ignoring those problems – without a solution to which he felt he could not live, – but trying to solve quite other problems which could not interest him, such as, for instance, the development of organisms, a mechanical explanation of the soul, and so on.\n",
            "TARGET: Un’altra cosa era che, avendo letto molti libri, s’era convinto che le persone le quali condividevano le sue opinioni non intendevano null’altro e, senza spiegar nulla, negavano non soltanto le questioni, senza la soluzione delle quali egli sentiva di non poter vivere, ma cercavano di risolvere questioni del tutto diverse, che non potevano interessarlo, come, per esempio, quella dell’evoluzione degli organismi, quella della spiegazione meccanica dell’anima e simili.\n",
            "PREDICTED: La cosa era stata , dopo aver cominciato a leggere un libro , che , in quel momento , egli stesso , aveva capito che cosa non aveva mai pensato a nessuno , e che non poteva capire nulla , ma che non poteva capire che cosa volesse dire , non poteva capire che cosa si poteva , ma che cosa si poteva fare , per la sua vita , e che non poteva essere per la sua salvezza , per la sua vita , per la sua vita , e la sua .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Really?' said Vronsky. 'She will be very glad.\n",
            "TARGET: — Davvero? — disse Vronskij. — Lei ne sarà molto lieta.\n",
            "PREDICTED: — Davvero ? — disse Vronskij . — molto .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 06:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 06: 100%|██████████| 3638/3638 [29:28<00:00,  2.06it/s, loss=3.645]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'You are married, I hear?' said the landowner.\n",
            "TARGET: — Vi siete ammogliato, ho sentito — disse il proprietario.\n",
            "PREDICTED: — Voi , che mi hai sentito ? — disse il proprietario .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Ah, so you are here!' she said on seeing him. 'Well, how is your poor sister?\n",
            "TARGET: — Ah, anche voi siete qui — ella disse nel vederlo. — Be’, come va la vostra povera sorella?\n",
            "PREDICTED: — Ah , siete qui ! — ella disse , guardando il tuo nome . — Su , come va ’ la tua sorella ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 07:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 07: 100%|██████████| 3638/3638 [29:26<00:00,  2.06it/s, loss=3.714]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: So it happens with fortune, who shows her power where valour has not prepared to resist her, and thither she turns her forces where she knows that barriers and defences have not been raised to constrain her.\n",
            "TARGET: Similmente interviene della fortuna: la quale dimonstra la sua potenzia dove non è ordinata virtù a resisterle, e quivi volta li sua impeti, dove la sa che non sono fatti li argini e li ripari a tenerla.\n",
            "PREDICTED: Così , con la fortuna , che la sua fortuna non ha più di non , e ha cercato di di , e che non si è più di , e che non si .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'N'est-ce pas immoral?' [Isn't it immoral?] was all she said after a pause.\n",
            "TARGET: — N’est-ce-pas immoral? — ella disse soltanto dopo un po’ di silenzio.\n",
            "PREDICTED: — C ’ est pas à ridicule ? — disse lei . — Non c ’ è un letto di silenzio .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 08:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 08: 100%|██████████| 3638/3638 [29:26<00:00,  2.06it/s, loss=3.895]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"You would often see him?\n",
            "TARGET: — Lo vedevate spesso?\n",
            "PREDICTED: — Avete spesso spesso a vedere ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Yashvin's face wore the expression it had when he was losing at cards.\n",
            "TARGET: Sul viso di Jašvin c’era l’espressione che soleva avere quando perdeva al giuoco.\n",
            "PREDICTED: Jašvin aveva l ’ espressione del viso di Jašvin quando era stato da mangiare .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 09:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 09: 100%|██████████| 3638/3638 [29:23<00:00,  2.06it/s, loss=3.018]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Already in February he had received a letter from Mary Nikolavna to say that his brother Nicholas's health was getting worse, but that he would not submit to any treatment. In consequence of this news Levin went to Moscow, saw his brother, and managed to persuade him to consult a doctor and go to a watering-place abroad.\n",
            "TARGET: Inoltre, in febbraio, aveva ricevuta da Mar’ja Nikolaevna una lettera in cui si diceva che le condizioni di salute del fratello erano peggiorate, e che egli non voleva curarsi; in seguito a questa lettera, Levin era andato a Mosca e aveva fatto in tempo a persuadere il fratello a consigliarsi con un medico e ad andare all’estero per la cura delle acque.\n",
            "PREDICTED: Durante il tè , era rimasto da una lettera da Mar ’ ja Nikolaevna che aveva parlato del fratello Nikolaj , ma che non temeva che fosse la notizia di Levin non potesse più la notizia di Mosca , e Levin si era allontanato verso di lui , e per un viaggio di cento rubli di un viaggio .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Jack and perch may BE about there.\n",
            "TARGET: Piccole lasche e perche forse ce ne sono.\n",
            "PREDICTED: Giacomo e Rebecca ai più infelici .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 10:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 10: 100%|██████████| 3638/3638 [29:21<00:00,  2.07it/s, loss=3.538]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: CHAPTER I\n",
            "TARGET: PARTE PRIMA\n",
            "PREDICTED: I\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Throw up everything and let us two conceal ourselves somewhere alone with our love,' said he to himself.\n",
            "TARGET: “Lei ed io dobbiamo abbandonare tutto e andarci a nascondere in qualche luogo, noi due, con il nostro amore” disse a se stesso.\n",
            "PREDICTED: — tutto e parleremo , perché ci soltanto due — disse , cercando di indovinare in precedenza che era lui .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 11:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 11: 100%|██████████| 3638/3638 [29:23<00:00,  2.06it/s, loss=3.101]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I want to stop here, leaning up against this gritty old wall.\n",
            "TARGET: Voglio star qui appoggiato a questo muro.\n",
            "PREDICTED: Avrei voglia di tornare indietro , con questa , al muro .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: More carriages kept driving up, and now ladies with flowers in their hair got out, holding up their trains; or men appeared who doffed their military caps or black hats as they entered the church.\n",
            "TARGET: Le carrozze si avvicinavano ininterrottamente, e ora signore con fiori e con gli strascichi sollevati, ora uomini che si toglievano il chepì o il cappello nero, entravano in chiesa.\n",
            "PREDICTED: Dietro alle signore si avvicinarono a loro , e ora , con la loro camicia , si afferrò con un movimento delle loro signore , si separarono e si come il Maškin Verch .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 12:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 12: 100%|██████████| 3638/3638 [29:25<00:00,  2.06it/s, loss=2.658]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: In uttering these words I looked up: he seemed to me a tall gentleman; but then I was very little; his features were large, and they and all the lines of his frame were equally harsh and prim.\n",
            "TARGET: Mi parve alto, ma mi ricordo che io allora ero molto piccola. I tratti di lui mi parvero molto marcati, e vi scorsi, come nelle linee di tutta la persona, una espressione di durezza e d'ipocrisia.\n",
            "PREDICTED: Quelle parole mi parvero parole e mi pareva che un signore era un poco debole , ma era una piccola faccia , e poi i tratti erano e i suoi tratti , che erano e il suo carattere .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Ah, so you are here!' she said on seeing him. 'Well, how is your poor sister?\n",
            "TARGET: — Ah, anche voi siete qui — ella disse nel vederlo. — Be’, come va la vostra povera sorella?\n",
            "PREDICTED: — Ah , siete così ! — ella disse , vedendo . — Be ’, come va la tua governante ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 13:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 13: 100%|██████████| 3638/3638 [29:23<00:00,  2.06it/s, loss=2.917]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: He did not undress, but paced up and down with his even step on the resounding parquet floor of the dining-room, which was lit by one lamp, over the carpet of the dark drawing-room, where a light was reflected only from a recently painted portrait of himself which hung above the sofa, and on through her sitting-room, where two candles were burning, lighting up the portraits of her relatives and friends and the elegant knick-knacks, long familiar to him, on her writing-table.\n",
            "TARGET: Senza essersi spogliato, andava avanti e indietro, con passo eguale, sul pavimento di legno scricchiolante della sala da pranzo illuminata da un’unica lampada, sul tappeto del salotto oscuro in cui la luce si rifletteva solo sul suo grande ritratto fatto da poco, appeso sopra il divano, e attraverso lo studiolo di lei, dove ardevano due candele che davan luce ai ritratti dei parenti e delle amiche e agli oggetti belli della scrivania a lui così noti da tempo.\n",
            "PREDICTED: Non appena scese giù , ma scese con passo veloce , sotto il portone le sedie sul tavolo , dal quale era illuminato da una lampada , un ’ angolo della stanza da pranzo , dove si svegliò un ’ acconciatura dove si era un ’ altra lampada ; e in un angolo di bronzo ancora due le preoccupazioni sottili , , , un portacenere rotto , un con le fotografie del servizio dello Yorkshire , e gli amici che gli si subito , in camera .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Nice story that, about your shirt!' said Sergius Ivanich with a smile and shake of the head.\n",
            "TARGET: — Carina la tua storia della camicia! — disse Sergej Ivanovic, scotendo la testa e sorridendo.\n",
            "PREDICTED: — Con questa bella — disse Sergej Ivanovic sorridendo e porgendo la capo alla capo .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 14:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 14: 100%|██████████| 3638/3638 [29:20<00:00,  2.07it/s, loss=3.056]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: This beautiful baby only inspired him with a sense of repulsion and pity.\n",
            "TARGET: Quel bellissimo bambino gli ispirava soltanto disgusto e pena.\n",
            "PREDICTED: Questo bambino solo con lui la propria impressione e con la pena .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Certainly count them!\n",
            "TARGET: — Contare, sì, assolutamente.\n",
            "PREDICTED: — .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 15:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 15: 100%|██████████| 3638/3638 [29:24<00:00,  2.06it/s, loss=2.407]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'I'm ready to agree with Arseny beforehand.\n",
            "TARGET: — Ma con Arsenij sono fin d’ora d’accordo su tutto.\n",
            "PREDICTED: — Sono già pronta a carte con Arsenij .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"Courage,\" urged the lawyer,--\"speak out.\"\n",
            "TARGET: — Coraggio! — continuò il legale, — parlate forte.\n",
            "PREDICTED: — — mi disse l ’ avvocato .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 16:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 16: 100%|██████████| 3638/3638 [29:26<00:00,  2.06it/s, loss=2.426]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I found in the low grounds hares (as I thought them to be) and foxes; but they differed greatly from all the other kinds I had met with, nor could I satisfy myself to eat them, though I killed several.\n",
            "TARGET: Trovai nelle terre basse e volpi e lepri, almeno così le giudicai; tanto diverse per altro da tutte le solite in cui m’era altrove abbattuto, che, se bene ne uccidessi molte, non seppi risolvermi ad assaggiarne.\n",
            "PREDICTED: Capii nel far delle eriche ( come potei ”, ma mi trovai molto ; su l ’ un l ’ altro , che nè potei nè mi di non aver mai avuta la speranza d ’ averne tuttavia altri , benchè dopo molto tempo di , eccetto pallini .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: A woman who could betray me for such a rival was not worth contending for; she deserved only scorn; less, however, than I, who had been her dupe.\n",
            "TARGET: \"Una donna che poteva antepormi un rivale come quello, non era degna di me, meritava soltanto di essere disprezzata; meno di me peraltro che ero stato ingannato da lei.\n",
            "PREDICTED: Una donna , che poteva servire a un tale accordo , non poteva non sperare nessun rispetto per me ; però non aveva che la sua occupazione .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 17:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 17: 100%|██████████| 3638/3638 [29:24<00:00,  2.06it/s, loss=2.543]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'After all, he is a good man: truthful, kind, and remarkable in his own sphere,' said Anna to herself when she had returned to her room, as if defending him from some one who accused him and declared it was impossible to love him. 'But why do his ears stick out so?\n",
            "TARGET: «Però è un brav’uomo, leale, di buon cuore e notevole nel suo campo — si andava dicendo Anna, tornata in camera sua; quasi a difenderlo di fronte a qualcuno che lo accusasse e che dicesse a lei che non lo si poteva amare. — Ma come mai ha le orecchie che gli sporgono così stranamente in fuori?\n",
            "PREDICTED: — Ma è buono , anima mia , gentile e , e in genere , del suo ambiente — disse ella , quando si fu presa da un ’ irritazione che , come se un pazzo di sollievo gli era entrato — e che il bambino gli dice di no , senza volere proprio perché il bastone ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I have only perceived what it is that I know.\n",
            "TARGET: Ho soltanto imparato a conoscere quello che sapevo.\n",
            "PREDICTED: Ma io non so a che cosa sia .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 18:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 18: 100%|██████████| 3638/3638 [29:19<00:00,  2.07it/s, loss=2.122]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: It is very old, and it was very strong and great once.\n",
            "TARGET: È città antichissima, e una volta era assai forte e grande.\n",
            "PREDICTED: È molto vecchio e molta di grande fatica .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: He vanished, but reappeared instantly--\n",
            "TARGET: Egli uscì e poco dopo ricomparve.\n",
            "PREDICTED: Egli si fece pensieroso , ma subito .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 19:   0%|          | 0/3638 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing Epoch 19: 100%|██████████| 3638/3638 [29:25<00:00,  2.06it/s, loss=2.301]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: George got hold of the paper, and read us out the boating fatalities, and the weather forecast, which latter prophesied \"rain, cold, wet to fine\" (whatever more than usually ghastly thing in weather that may be), \"occasional local thunder-storms, east wind, with general depression over the Midland Counties (London and Channel).\n",
            "TARGET: Giorgio s’impadronì del giornale, e ci lesse le disgrazie fluviali e marittime, e la previsione del tempo, che vaticinava «pioggia, freddo, vento» (tutto ciò che ci può esser di peggiore nel tempo), e qualche temporale locale, con depressione generale sulle contee centrali (Londra e Canale).\n",
            "PREDICTED: Giorgio della carta , e ci dirigemmo a sentire la barca , e i racconti delle barche che nelle pozzanghere , nelle notti più lunghe ( più si può dire che si può fare una qualche tempo ), che si può scegliere una e un po ’ di sole , più durante la stagione delle torte , un po ’ più di cinque anni e un po ’ di sole .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'But I understood that Anna Arkadyevna declined a divorce if I insisted on keeping my son.\n",
            "TARGET: — Ma io ritenevo che Anna Arkad’evna rinunciasse al divorzio nel caso che io esigessi l’obbligo di lasciarmi il figlio.\n",
            "PREDICTED: — Ma io ho capito che Anna Arkad ’ evna ha voluto chiedere al divorzio .\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "config = get_config()\n",
        "train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5551c4b4",
      "metadata": {
        "papermill": {
          "duration": 6.070149,
          "end_time": "2025-06-10T10:12:41.700942",
          "exception": false,
          "start_time": "2025-06-10T10:12:35.630793",
          "status": "completed"
        },
        "tags": [],
        "id": "5551c4b4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 35474.772773,
      "end_time": "2025-06-10T10:12:51.803324",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-10T00:21:37.030551",
      "version": "2.6.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}